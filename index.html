<html>
	<head>
	<script src="https://www.google.com/jsapi" type="text/javascript"></script>
	<script type="text/javascript">google.load("jquery", "1.3.2");</script>
	
	
	<style type="text/css">
		body {
			font-family: 'Noto Sans', sans-serif;
			color: #4a4a4a;
			font-size: 1em;
			font-weight: 400;
			line-height: 1.5;
			margin-left: auto;
			margin-right: auto;
			width: 1100px;
		}
	
		.title {
			color: #363636;
			font-size: 4rem;
			line-height: 1.125;
			font-weight: 200;
			font-family: 'Google Sans', sans-serif;
		}
	
		.publication-title {
			font-family: 'Google Sans', sans-serif;
		}
	
		h1 {
			font-size: 40px;
			font-weight: 500;
		}
	
		h2 {
			font-size: 35px;
			font-weight: 300;
		}
	
		h3 {
			font-size: 1px;
			font-weight: 300;
		}
	
		.subtitle,
		.title {
			word-break: break-word;
			font-size:3rem;
		}
	
		.title.is-2 {
			font-size: 4rem;
			font-weight: 400;
		}
	
		.title.is-3 {
			font-size: 2.5rem;
			font-weight: 400;
		}
	
		.content h2 {
			font-weight: 600;
			font-family: 'Noto Sans', sans-serif;
			line-height: 1.125;
		}
	
		.disclaimerbox {
			background-color: #eee;
			border: 1px solid #eeeeee;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			padding: 20px;
		}
	
		video.header-vid {
			height: 140px;
			border: 1px solid rgba(0, 0, 0, 0.212);
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
		}
	
		img.header-img {
			height: 140px;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
		}
	
		img.rounded {
			border: 1px solid #eeeeee;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
		}
	
		a:link,
		a:visited {
			color: #2994c5;
			text-decoration: none;
		}
	
		a:hover {
			color: #0e889e;
		}
	
		td.dl-link {
			height: 160px;
			text-align: center;
			font-size: 22px;
		}
	
		.layered-paper-big {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35),
				/* The third layer shadow */
				15px 15px 0 0px #fff,
				/* The fourth layer */
				15px 15px 1px 1px rgba(0, 0, 0, 0.35),
				/* The fourth layer shadow */
				20px 20px 0 0px #fff,
				/* The fifth layer */
				20px 20px 1px 1px rgba(0, 0, 0, 0.35),
				/* The fifth layer shadow */
				25px 25px 0 0px #fff,
				/* The fifth layer */
				25px 25px 1px 1px rgba(0, 0, 0, 0.35);
			/* The fifth layer shadow */
			margin-left: 10px;
			margin-right: 45px;
		}
	
		.paper-big {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35);
			/* The top layer shadow */
	
			margin-left: 10px;
			margin-right: 45px;
		}
	
		.button.is-dark {
			background-color: #363636;
			border-color: transparent;
			color: #fff;
		}
	
		.button.is-rounded {
			border-radius: 290486px;
			padding-left: calc(1em + .25em);
			padding-right: calc(1em + .25em);
		}
	
		.button.is-normal {
			font-size: 1rem;
		}
	
		.button .icon,
		.button .icon.is-large,
		.button .icon.is-medium,
		.button .icon.is-small {
			height: 1.5em;
			width: 1.5em;
		}
	
		.icon {
			align-items: center;
			display: inline-flex;
			justify-content: center;
			height: 1.5rem;
			width: 1.5rem;
		}
	
		.button.is-normal {
			font-size: 1rem;
		}
	
		.button {
			background-color: #fff;
			border-color: #dbdbdb;
			border-width: 1px;
			color: #363636;
			cursor: pointer;
			justify-content: center;
			padding-bottom: calc(.5em - 1px);
			padding-left: 1em;
			padding-right: 1em;
			padding-top: calc(.5em - 1px);
			text-align: center;
			white-space: nowrap;
		}
	
		.button .icon:first-child:not(:last-child) {
			margin-left: calc(-.5em - 1px);
			margin-right: .25em;
		}
		
		.button.is-rounded {
			border-radius: 290486px;
			padding-left: calc(1em + .25em);
			padding-right: calc(1em + .25em);
		}
	
		.button.is-normal {
			font-size: 1rem;
		}
	
		.button.is-dark {
			background-color: #363636;
			border-color: transparent;
			color: #fff;
		}
	
		.link-block a {
			margin-top: 15px;
			margin-bottom: 15px;
		}
	
		.button {
			background-color: #fff;
			border-color: #dbdbdb;
			border-width: 1px;
			color: #363636;
			cursor: pointer;
			justify-content: center;
			padding-bottom: calc(.5em - 1px);
			padding-left: 1em;
			padding-right: 1em;
			padding-top: calc(.5em - 1px);
			text-align: center;
			white-space: nowrap;
		}
	
		.button,
		.file-cta,
		.file-name,
		.input,
		.pagination-ellipsis,
		.pagination-link,
		.pagination-next,
		.pagination-previous,
		.select select,
		.textarea {
			-moz-appearance: none;
			-webkit-appearance: none;
			align-items: center;
			border: 1px solid transparent;
			border-top-color: transparent;
			border-top-width: 1px;
			border-right-color: transparent;
			border-right-width: 1px;
			border-bottom-color: transparent;
			border-bottom-width: 1px;
			border-left-color: transparent;
			border-left-width: 1px;
			border-radius: 4px;
			box-shadow: none;
			display: inline-flex;
			font-size: 1rem;
			height: 2.5em;
			justify-content: flex-start;
			line-height: 1.5;
			padding-bottom: calc(.5em - 1px);
			padding-left: calc(.75em - 1px);
			padding-right: calc(.75em - 1px);
			padding-top: calc(.5em - 1px);
			position: relative;
			vertical-align: top;
		}
	
		.breadcrumb,
		.button,
		.delete,
		.file,
		.is-unselectable,
		.modal-close,
		.pagination-ellipsis,
		.pagination-link,
		.pagination-next,
		.pagination-previous,
		.tabs {
			-webkit-touch-callout: none;
			-webkit-user-select: none;
			-moz-user-select: none;
			-ms-user-select: none;
			user-select: none;
		}
	
		a {
			color: #3273dc;
			cursor: pointer;
			text-decoration: none;
		}
	
		a {
			color: #007bff;
			text-decoration: none;
			background-color: transparent;
		}
	
		*,
		::after,
		::before {
			box-sizing: inherit;
		}
	
		*,
		::before,
		::after {
			box-sizing: border-box;
		}
	
		span {
			font-style: inherit;
			font-weight: inherit;
		}
	
		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}
	
		.vert-cent {
			position: relative;
			top: 50%;
			transform: translateY(-50%);
		}
	
		hr {
			border: 0;
			height: 1px;
			background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
		}
	</style>
	
	<title>Visual Optimization: Auto-shortening and Dynamic ROI Cropping</title>
	<meta property="og:image" content="./assets/teaser_vpp_v2.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Active Stereo Without Pattern Projector" />
	<meta property="og:url" content="https://vppstereo.github.io/">
	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
  		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	<!-- <script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
	</script> -->

</head>

<body>
	<br>
	<center>
		<h1 class="title publication-title" style="margin-bottom: 0"><strong>Streamlining auto-shortened video
			content with ROI cropping and aspect-ratio modification
		</strong></h1>
		<table>
			<!-- Code Link. -->
			<span class="link-block">
				<a href="https://github.com/Shershon25/FYP_PRJ_V1" class="external-link button is-normal is-rounded is-dark"
					style="background-color:#000000;">
					<span class="icon">
						<svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false"
							data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg"
							viewBox="0 0 496 512" data-fa-i2svg="">
							<path fill="currentColor"
								d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
							</path>
						</svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
					</span>
					<span>Code</span>
				</a>
			</span>
		</table>
		<table align=center width="1100px">
			<table align=center width="1000px">
				<tr>
					<td align=center width="180px">
						<center>
							<span style="font-size:25px"><a href="https://github.com/1Nisha5">
									Nisha Pa</a></span>
						</center>
					</td>
					<td align=center width="150px">
						<center>
							<span style="font-size:25px"><a href="https://github.com/EVR005">
									Edumba Vannia Raja</a></span>
						</center>
					</td>
					<td align=center width="150px">
						<center>
							<span style="font-size:25px"><a href="https://github.com/Shershon25">
									Shershon A J</a></span>
						</center>
					</td>
				</tr>
			</table>
			
	</center>
	<br>
	<!-- <center>
		<table align=center width="1000px">
			<tr>
				<td>
					<center>
						<img class="round" width="1000px" src="./assets/teaser_vpp_v2.png" />
					</center>
				</td>
			</tr>
		</table>
		<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
			<tr>
				<td>
					<p style="text-align: justify;">
						<strong>Virtual Pattern Projection for deep stereo.</strong> Either in challenging outdoor (top) or indoor (bottom) environments (a), a stereo network such as PSMNet often struggles (b). 
						By projecting a virtual pattern on images (c), the very same network dramatically improves its accuracy (d). 
						By further training the model to deal with the augmented images (e) further improves the results.
					</p>
				</td>
			</tr>
		</table>
	</center> -->

	<center>
		<h1>Abstract</h1>
	</center>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<p style="text-align: justify;">
					<i>This project aims to provide a shortened video with respect to an Object of Interest(OoI), which can be viewed on any device based on requirements. The input to this model can be any video of extensive length that can be difficult for manual processing. The input video frames are selected based on the presence of the Object of Interest, which is taken from the user as a text input or a segmented object from any of the video's frames. The frames thus filtered will be marked such that the Object of Interest falls inside the boundary markers. Such modified frames will be fed into a shortening algorithm such that redundant frames are removed from them. The timestamp of the frames with respect to the original video is marked and the aspect ratio of the frames is modified as per the user's request. This project can be valuable to many real-world problems like surveillance footage analysis</i>
				</p>
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<center>
		<h1>Method</h1>
	</center>
	
	<br>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<center>
			<tr>
				<td>
					<h2> 1 - Problems</h2>
					<ol>
						<li>
							<p style="text-align: justify;">Shortening of large videos as in the case of surveillance systems imposes a heavy time complexity on processing the entire length of the video, while during the shortening process itself.</p>
							<!-- <table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td><img src="assets/vanilla_midd21.png" width="495px" /></td>
									<td><img src="assets/vanilla_midd21_cfnet.png" width="495px" /></td>
								</tr>
							</table>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td>
										<p style="text-align: justify;">
											<strong>Performance on uniform areas.</strong> Even <a href="https://github.com/gallenszl/CFNet">recent stereo network</a> struggles with uniform areas.
										</p>
									</td>
								</tr>
							</table> -->
						</li>
						<li>
							<p style="text-align: justify;">
								Cropping the frames of the video so that the concerned objects are focussed and choosing a global frame aspect ratio with respect to the entire video, so as to produce a high detailed video and also facilitate easier processing of details from the same.
							</p>
							<!-- <center>
								<img src="assets/active_pattern.gif" width="1000px" />
							</center>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td>
										<p style="text-align: justify;">
											<strong>Performance of active pattern.</strong> Even in a indoor scenario, active light decreases proportionally to the square of the distance. As soon as we move in an external environment, the projected light is dimmed by the external light.
										</p>
									</td>
								</tr>
							</table> -->
						</li>
						<li><p style="text-align: justify;">Detecting a single object from a frame, where other objects similar to the former are also present and if a group of objects are required, then drawing bounding boxes altogether around the concerned objects is a complexity worth to be noted, here.</p></li>
						<li><p style="text-align: justify;">Employing the use of efficient object tracking algorithms so as to skip unnecessary frames with absolutely null or little change and thereby reduce the video processing complexity and also analysing the cases where this technique can be applied for better results.</p></li>
					</ol>
				</td>
			</tr>
		</center>
	</table>

	<br>
	<br>
	<hr>

	<center>
		<h1>Architecture Diagram</h1>
	</center>
	<center>
		<img src="assets/images/architecture.png" width="1000px"/>
	</center>
	<br>
	<br>
	<hr>



	<center>
		<h1>System Design</h1>
	</center>
	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<h2>1 - Video Frame Extraction</h2>
				<p style="text-align: justify;">
					The initial step in video frame extraction is getting input video from the 
user. We have chosen Gradio as the UI framework as Gradio specifies no limit on 
the size of the input video. The user will be uploading the video through the upload 
video interface. The uploaded video’s characteristics are displayed in the User 
interface. This includes FPS of the given video, video frame’s height and width, 
name of the video file and the number of frames. Then the user will be specifying 
the range of frames that they want to look for the object.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/1.jpg" width="1000px"/>
	</center>
	<center>
		<h5>Specifying start and end frames
		</h5>
	</center>
	<br>




	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<!-- <h2>Video Frame Extraction</h2> -->
				<p style="text-align: justify;">
					From the extracted frames, the user will be selecting a frame that will be 
used for the mask generation part. The output of the frame selection will be 
handled by the SegmentAnything model.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/2.png" width="1000px"/>
	</center>
	<center>
		<h5>Frame Selection
		</h5>
	</center>
	<br>
	<br>
			<hr>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<h2>2 - Mask Generation using SAM</h2>
				<p style="text-align: justify;">
					The system is designed to generate keyframe masks using the 
Segment Anything model, specifically incorporating the VIT-H checkpoint model 
for mask generation. It begins by taking video frames as input and subjecting them 
to preprocessing, which may include noise reduction and color correction.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/3.png" width="1000px"/>
	</center>
	<center>
		<h5>Video Frame input
		</h5>
	</center>
	<br>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<!-- <h2>Video Frame Extraction</h2> -->
				<p style="text-align: justify;">
					Once masks for each frame are generated, the user is prompted to select a 
point of interest within the image. Clicking on the point leads to the identification 
and display of masks where the selected point resides.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/4.png" width="1000px"/>
	</center>
	<center>
		<h5>All Masks at a particular point
		</h5>
	</center>
	<br>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<!-- <h2>Video Frame Extraction</h2> -->
				<p style="text-align: justify;">
					Once masks for each frame are generated, the user is prompted to select a 
point of interest within the image. Clicking on the point leads to the identification 
and display of masks where the selected point resides.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/4.png" width="1000px"/>
	</center>
	<center>
		<h5>All Masks at a particular point
		</h5>
	</center>
	<br>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<!-- <h2>Video Frame Extraction</h2> -->
				<p style="text-align: justify;">
					The user then selects the desired mask, which is considered a keyframe. 
The system identifies frames in which this keyframe mask is present, and these 
frames are extracted as keyframes. The output consists of the selected keyframes, 
each containing the key object of interest, enabling efficient content extraction for 
various applications such as video editing and content summarization.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/5.png" width="1000px"/>
	</center>
	<center>
		<h5>Selected mask of the object
		</h5>
	</center>
	<br>
			<br>
			<hr>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<h2>3 - Video Object Segmentation using XMem</h2>
				<p style="text-align: justify;">
					The XMem model operates by integrating multiple memory stores and 
dynamic memory management. It consists of three key memory components: 
sensory memory, working memory, and long-term memory. Sensory memory 
rapidly updates information from video frames, while working memory retains 
high-resolution details. Long-term memory stores compact representations.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/6.png" width="1000px"/>
	</center>
	<center>
		<h5>Generated Mask for an object
		</h5>
	</center>
	<br>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<!-- <h2>Video Frame Extraction</h2> -->
				<p style="text-align: justify;">
					Dynamic memory management plays a pivotal role in the XMem model. It 
employs a memory potentiation algorithm that periodically consolidates actively 
used information from working memory into the long-term memory. This 
prevents memory overload and ensures efficient memory usage over extended 
video sequences.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<img src="assets/images/7.png" width="1000px"/>
	</center>
	<center>
		<h5>Object Segmented Video
		</h5>
	</center>
	<br>
	<br>
	<hr>

	<center><h1>Future Works</h1></center>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<!-- <h2>Video Frame Extraction</h2> -->
				<p style="text-align: justify;">
					In the realm of video analysis and summarization, the current project has laid a 
solid foundation by successfully extracting frames from videos based on user-specified ranges, allowing users to select keyframes and generate object masks using Segment 
Anything. The subsequent integration of an XMem model for object tracking across the 
video, coupled with the frame skipping model to eliminate redundant frames, has 
enhanced the efficiency of video summarization. Looking ahead, the project envisions 
several exciting avenues for future development and improvement.

				</p>
			</td>
			<br>
			<br>
		</tr>
	</table>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
	<tr>
		<td>
			<!-- <h2>Video Frame Extraction</h2> -->
			<p style="text-align: justify;">
				Firstly, the future iterations of the model will focus on extending its capabilities 
to accommodate multiple object tracking. This enhancement will enable the system to 
simultaneously monitor and trace the trajectories of multiple objects within a video, 
providing a more comprehensive understanding of dynamic scenes. Additionally, the 
integration of multi-view video input will be explored to further enhance the 
summarization process. By incorporating multiple perspectives, the model can generate 
more nuanced and informative summaries, offering a richer representation of the 
content.

			</p>
		</td>
		<br>
		<br>
	</tr>
	</table>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<!-- <h2>Video Frame Extraction</h2> -->
				<p style="text-align: justify;">
					Furthermore, a significant future direction involves the development of a 
mechanism to generate textual summaries of the shortened videos. This addition will 
bring a new dimension to the project, as it aims to provide users with not only a concise 
visual representation but also a textual synopsis that encapsulates the key events and 
information within the video. This textual summary can serve as a valuable tool for users 
who prefer or require a textual overview, broadening the accessibility and usability of
the system.
	
				</p>
			</td>
			<br>
			<br>
		</tr>
		</table>

		<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
			<tr>
				<td>
					<!-- <h2>Video Frame Extraction</h2> -->
					<p style="text-align: justify;">
						Thus, the future works for this project are poised to advance the state-of-the-art 
in video summarization by incorporating multiple object tracking, multi-view video 
input, and the generation of textual summaries. These enhancements will not only 
contribute to the project's technical robustness but also extend its practical applicability 
in various domains, ranging from surveillance and security to content creation and 
archival.

		
					</p>
				</td>
				<br>
				<br>
			</tr>
			</table>

			<br>
			<br>
			<hr>
</body>

</html>
